#!/usr/bin/env python3
"""
æ‰‹ç¶šããƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å°‚ç”¨ä½œæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""
import os
import sys

# ç’°å¢ƒå¤‰æ•°ã¨ãƒ‘ã‚¹è¨­å®š
sys.path.append('/Users/akamatsu/Desktop/ai-agent/src')
os.chdir('/Users/akamatsu/Desktop/ai-agent')

from dotenv import load_dotenv
load_dotenv()

from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

def create_procedures_database():
    """æ‰‹ç¶šããƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä½œæˆ"""
    print("ğŸ”§ æ‰‹ç¶šããƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä½œæˆä¸­...")
    
    # OpenAI EmbeddingsåˆæœŸåŒ–
    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-small"
    )
    
    # æ‰‹ç¶šããƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ï¼ˆä¸¡æ–¹ã®ãƒ‘ã‚¹ã‚’ãƒã‚§ãƒƒã‚¯ï¼‰
    procedures_paths = [
        "data/procedures",
        "src/data/procedures"
    ]
    
    all_documents = []
    
    for path in procedures_paths:
        if os.path.exists(path):
            print(f"ğŸ“ {path} ã‹ã‚‰èª­ã¿è¾¼ã¿ä¸­...")
            loader = DirectoryLoader(
                path,
                glob="*.txt",
                loader_cls=TextLoader,
                loader_kwargs={'encoding': 'utf-8'}
            )
            docs = loader.load()
            all_documents.extend(docs)
            print(f"   -> {len(docs)} ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    
    if not all_documents:
        print("âŒ æ‰‹ç¶šããƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return False
    
    print(f"ğŸ“„ ç·ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {len(all_documents)}")
    
    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†…å®¹ç¢ºèª
    for i, doc in enumerate(all_documents):
        print(f"   ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ {i+1}: {doc.metadata.get('source', 'Unknown')}")
        print(f"   å†…å®¹ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: {doc.page_content[:100]}...")
    
    # ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len,
    )
    
    texts = text_splitter.split_documents(all_documents)
    print(f"ğŸ“„ åˆ†å‰²ã—ãŸãƒ†ã‚­ã‚¹ãƒˆæ•°: {len(texts)}")
    
    # FAISSãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ä½œæˆ
    vectorstore = FAISS.from_documents(texts, embeddings)
    
    # srcãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜
    save_path = "src/faiss_index_procedures"
    vectorstore.save_local(save_path)
    print(f"âœ… æ‰‹ç¶šããƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {save_path}")
    
    # ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚‚ã‚³ãƒ”ãƒ¼
    root_save_path = "faiss_index_procedures"
    vectorstore.save_local(root_save_path)
    print(f"âœ… æ‰‹ç¶šããƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸ: {root_save_path}")
    
    return True

if __name__ == "__main__":
    success = create_procedures_database()
    if success:
        print("\nğŸ‰ æ‰‹ç¶šããƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ä½œæˆãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print("ğŸ’¡ ã‚µãƒ¼ãƒãƒ¼ã‚’å†èµ·å‹•ã—ã¦æ–°ã—ã„æƒ…å ±ã‚’é©ç”¨ã—ã¦ãã ã•ã„ã€‚")
    else:
        print("\nâŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
