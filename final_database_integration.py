#!/usr/bin/env python3
"""
æœ€çµ‚ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±åˆ - ç”°ä¸­ã•ã‚“ã®æƒ…å ±ã‚’ç¢ºå®Ÿã«å«ã‚ã‚‹
"""
import os
import sys

sys.path.append('/Users/akamatsu/Desktop/ai-agent/src')
os.chdir('/Users/akamatsu/Desktop/ai-agent')

from dotenv import load_dotenv
load_dotenv()

from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

def final_database_integration():
    """æœ€çµ‚çš„ãªãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±åˆ"""
    print("ğŸ”„ æœ€çµ‚ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±åˆã‚’å®Ÿè¡Œä¸­...")
    
    # OpenAI EmbeddingsåˆæœŸåŒ–
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    
    # æ—¢å­˜ã®äº‹å‹™è¦å®šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’å‰Šé™¤
    import shutil
    office_db_paths = [
        "src/faiss_index_office", 
        "faiss_index_office"
    ]
    
    for path in office_db_paths:
        if os.path.exists(path):
            shutil.rmtree(path)
            print(f"ğŸ—‘ï¸ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’å‰Šé™¤: {path}")
    
    # 1. äº‹å‹™è¦å®šãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    all_documents = []
    
    # äº‹å‹™è¦å®šãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
    office_docs_path = "src/data/office_docs"
    if os.path.exists(office_docs_path):
        office_loader = DirectoryLoader(
            office_docs_path,
            glob="**/*.md",
            loader_cls=TextLoader,
            loader_kwargs={'encoding': 'utf-8'}
        )
        office_docs = office_loader.load()
        all_documents.extend(office_docs)
        print(f"ğŸ“ äº‹å‹™è¦å®šãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: {len(office_docs)} ãƒ•ã‚¡ã‚¤ãƒ«")
    
    # 2. æ‰‹ç¶šããƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆæ”¹å–„ç‰ˆï¼‰
    procedures_paths = ["data/procedures", "src/data/procedures"]
    
    for path in procedures_paths:
        if os.path.exists(path):
            procedures_loader = DirectoryLoader(
                path,
                glob="*.txt",
                loader_cls=TextLoader,
                loader_kwargs={'encoding': 'utf-8'}
            )
            docs = procedures_loader.load()
            all_documents.extend(docs)
            print(f"ğŸ“ {path}: {len(docs)} ãƒ•ã‚¡ã‚¤ãƒ«")
    
    print(f"ğŸ“„ ç·ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {len(all_documents)}")
    
    if not all_documents:
        print("âŒ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return False
    
    # 3. æ‰‹ç¶šããƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ç¢ºèª
    for doc in all_documents:
        if "paid_leave_procedure.txt" in doc.metadata.get("source", ""):
            print(f"ğŸ“‹ æœ‰çµ¦æ‰‹ç¶šããƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ç¢ºèª:")
            content_preview = doc.page_content[:300]
            print(f"   å†…å®¹ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: {content_preview}...")
            if "ç”°ä¸­ã•ã‚“" in doc.page_content:
                print("   âœ… ç”°ä¸­ã•ã‚“ã®æƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ã¾ã™")
            if "ãƒ¡ãƒ¼ãƒ«é€ä¿¡" in doc.page_content:
                print("   âœ… ãƒ¡ãƒ¼ãƒ«é€ä¿¡æ©Ÿèƒ½ã®æ¡ˆå†…ãŒå«ã¾ã‚Œã¦ã„ã¾ã™")
    
    # 4. å°ã•ãªãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã§åˆ†å‰²ï¼ˆé‡è¦æƒ…å ±ã®ä¿æŒï¼‰
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,  # ã‚ˆã‚Šå°ã•ã
        chunk_overlap=300,  # ã‚ˆã‚Šå¤§ããªã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—
        length_function=len,
    )
    texts = text_splitter.split_documents(all_documents)
    print(f"ğŸ“„ åˆ†å‰²ãƒ†ã‚­ã‚¹ãƒˆæ•°: {len(texts)}")
    
    # 5. é‡è¦ãƒãƒ£ãƒ³ã‚¯ã®ç¢ºèª
    important_chunks = 0
    for i, text in enumerate(texts):
        if "ç”°ä¸­ã•ã‚“" in text.page_content and "å†…ç·š" in text.page_content:
            important_chunks += 1
            print(f"   âœ… ãƒãƒ£ãƒ³ã‚¯ {i}: ç”°ä¸­ã•ã‚“ã®é€£çµ¡å…ˆæƒ…å ±ã‚’å«ã‚€")
        if "ãƒ¡ãƒ¼ãƒ«é€ä¿¡" in text.page_content and "è‡ªå‹•" in text.page_content:
            print(f"   âœ… ãƒãƒ£ãƒ³ã‚¯ {i}: ãƒ¡ãƒ¼ãƒ«é€ä¿¡æ©Ÿèƒ½ã®æ¡ˆå†…ã‚’å«ã‚€")
    
    print(f"ğŸ“§ ç”°ä¸­ã•ã‚“ã®æƒ…å ±ã‚’å«ã‚€ãƒãƒ£ãƒ³ã‚¯æ•°: {important_chunks}")
    
    # 6. ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä½œæˆ
    vectorstore = FAISS.from_documents(texts, embeddings)
    
    # 7. ä¿å­˜
    vectorstore.save_local("src/faiss_index_office")
    vectorstore.save_local("faiss_index_office")
    print("âœ… æœ€çµ‚çµ±åˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä¿å­˜ã—ã¾ã—ãŸ")
    
    return True

if __name__ == "__main__":
    success = final_database_integration()
    if success:
        print("\nğŸ‰ æœ€çµ‚ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±åˆãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print("\nğŸ“‹ æ”¹å–„å†…å®¹:")
        print("   - ç”°ä¸­ã•ã‚“ã®é€£çµ¡å…ˆæƒ…å ±ãŒæœ€ä¸Šä½ã«é…ç½®")
        print("   - ãƒ¡ãƒ¼ãƒ«é€ä¿¡æ©Ÿèƒ½ãŒç›®ç«‹ã¤ä½ç½®ã«é…ç½®")
        print("   - å°ã•ãªãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã§é‡è¦æƒ…å ±ã‚’ä¿æŒ")
        print("   - åŸºæœ¬çš„ãªè³ªå•ã§ã‚‚è©³ç´°ã‚µãƒãƒ¼ãƒˆæƒ…å ±ã‚’å«ã‚€")
        print("\nğŸ’¡ ã‚µãƒ¼ãƒãƒ¼ã‚’å†èµ·å‹•ã—ã¦ãƒ†ã‚¹ãƒˆã—ã¦ãã ã•ã„ã€‚")
    else:
        print("\nâŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®çµ±åˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
