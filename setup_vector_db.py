# setup_vector_db.py (äº‹å‹™ä½œæ¥­å°‚ç”¨ç§˜æ›¸ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç‰ˆ)
import os
import sys
from dotenv import load_dotenv

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€
load_dotenv()

# srcãƒ•ã‚©ãƒ«ãƒ€ã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

# å¿…è¦ãªã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import TextLoader, DirectoryLoader

def build_and_save_dbs():
    """
    ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ§‹ç¯‰ã—ã€ãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ (äº‹å‹™ä½œæ¥­å°‚ç”¨ç‰ˆ)
    """
    print("äº‹å‹™ä½œæ¥­å°‚ç”¨AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ§‹ç¯‰ã—ã¾ã™...")
    
    # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å–å¾—
    script_dir = os.path.dirname(os.path.abspath(__file__))
    os.chdir(script_dir)  # ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å ´æ‰€ã«å¤‰æ›´
    
    # OpenAIã®Embeddingãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®š
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

    try:
        # --- è²©å£²ä¼šè­°è³‡æ–™ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æ§‹ç¯‰ ---
        print("\n--- è²©å£²ä¼šè­°è³‡æ–™ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æ§‹ç¯‰ã‚’é–‹å§‹ ---")
        print("FAISS(è²©å£²ä¼šè­°è³‡æ–™)ã‚’åˆæœŸåŒ–ä¸­...")
        
        # è²©å£²ä¼šè­°è³‡æ–™ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        sales_data_path = "data/sales_meeting_data.txt"
        if os.path.exists(sales_data_path):
            loader_sales = TextLoader(sales_data_path, encoding='utf-8')
            docs_sales = loader_sales.load()
            
            text_splitter_sales = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=100
            )
            texts_sales = text_splitter_sales.split_documents(docs_sales)
            
            for i, doc in enumerate(texts_sales, 1):
                print(f"  -> è²©å£²ä¼šè­°è³‡æ–™ ãƒãƒ£ãƒ³ã‚¯ {i}/{len(texts_sales)} ã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚")
            
            db_sales = FAISS.from_documents(texts_sales, embeddings)
            db_sales.save_local("faiss_index_sales")
            print("âœ… è²©å£²ä¼šè­°è³‡æ–™ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ 'faiss_index_sales' ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
        else:
            print("âŒ è²©å£²ä¼šè­°è³‡æ–™ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

        # --- äº‹å‹™è¦å®šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æ§‹ç¯‰ ---
        print("\n--- äº‹å‹™è¦å®šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æ§‹ç¯‰ã‚’é–‹å§‹ ---")
        print("FAISS(äº‹å‹™è¦å®š)ã‚’åˆæœŸåŒ–ä¸­...")
        
        loader_office = DirectoryLoader(
            "src/data/office_docs/", 
            glob="**/*.md", 
            loader_cls=TextLoader, 
            loader_kwargs={'encoding': 'utf-8'}
        )
        docs_office = loader_office.load()
        
        if docs_office:
            text_splitter_office = RecursiveCharacterTextSplitter(
                chunk_size=1000, 
                chunk_overlap=100
            )
            texts_office = text_splitter_office.split_documents(docs_office)
            
            for i, doc in enumerate(texts_office, 1):
                print(f"  -> äº‹å‹™è¦å®šæ–‡æ›¸ {i}/{len(texts_office)} ã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚")
            
            db_office = FAISS.from_documents(texts_office, embeddings)
            db_office.save_local("faiss_index_office")
            print("âœ… äº‹å‹™è¦å®šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ 'faiss_index_office' ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
        else:
            print("âŒ äº‹å‹™è¦å®šæ–‡æ›¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

        # --- æ‰‹ç¶šãã‚¬ã‚¤ãƒ‰ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æ§‹ç¯‰ ---
        print("\n--- æ‰‹ç¶šãã‚¬ã‚¤ãƒ‰ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æ§‹ç¯‰ã‚’é–‹å§‹ ---")
        print("FAISS(æ‰‹ç¶šãã‚¬ã‚¤ãƒ‰)ã‚’åˆæœŸåŒ–ä¸­...")
        
        # Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        loader_procedures_md = DirectoryLoader(
            "src/data/procedures_docs/", 
            glob="**/*.md", 
            loader_cls=TextLoader, 
            loader_kwargs={'encoding': 'utf-8'}
        )
        docs_procedures_md = loader_procedures_md.load()
        
        # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚‚èª­ã¿è¾¼ã¿
        loader_procedures_txt = DirectoryLoader(
            "data/procedures/", 
            glob="**/*.txt", 
            loader_cls=TextLoader, 
            loader_kwargs={'encoding': 'utf-8'}
        )
        docs_procedures_txt = loader_procedures_txt.load()
        
        # ä¸¡æ–¹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’çµåˆ
        docs_procedures = docs_procedures_md + docs_procedures_txt
        
        if docs_procedures:
            text_splitter_procedures = RecursiveCharacterTextSplitter(
                chunk_size=1000, 
                chunk_overlap=100
            )
            texts_procedures = text_splitter_procedures.split_documents(docs_procedures)
            
            for i, doc in enumerate(texts_procedures, 1):
                print(f"  -> æ‰‹ç¶šãã‚¬ã‚¤ãƒ‰æ–‡æ›¸ {i}/{len(texts_procedures)} ã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚")
            
            db_procedures = FAISS.from_documents(texts_procedures, embeddings)
            db_procedures.save_local("faiss_index_procedures")
            print("âœ… æ‰‹ç¶šãã‚¬ã‚¤ãƒ‰ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ 'faiss_index_procedures' ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
        else:
            print("âŒ æ‰‹ç¶šãã‚¬ã‚¤ãƒ‰æ–‡æ›¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        
        print("\nğŸ‰ äº‹å‹™ä½œæ¥­å°‚ç”¨AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print("\nğŸ“‹ æ§‹ç¯‰ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹:")
        print("  - faiss_index_sales (è²©å£²ä¼šè­°è³‡æ–™)")
        print("  - faiss_index_office (äº‹å‹™è¦å®š)")
        print("  - faiss_index_procedures (æ‰‹ç¶šãã‚¬ã‚¤ãƒ‰ - MD & TXTãƒ•ã‚¡ã‚¤ãƒ«å«ã‚€)")

    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    build_and_save_dbs()